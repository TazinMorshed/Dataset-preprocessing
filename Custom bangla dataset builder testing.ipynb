{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from torchvision import transforms, datasets\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "\n",
    "class OCRDataset():\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.inp_h = 32\n",
    "        self.inp_w = 128\n",
    "       # print(\"image_dir: \" + img_dir)\n",
    "\n",
    "        self.img_names = sorted(os.listdir(\n",
    "            img_dir), key=lambda x: str(x.split('.')[0]))\n",
    "\n",
    "        self.img_names = sorted(self.img_names, key=numericalSort)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        image = cv2.imread(os.path.join(self.img_dir, img_name))\n",
    "        #print(os.path.join(self.img_dir, img_name))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        img_h, img_w = image.shape\n",
    "        image = cv2.resize(image, (0, 0), fx=self.inp_w / img_w,\n",
    "                           fy=self.inp_h / img_h, interpolation=cv2.INTER_CUBIC)\n",
    "        image = np.reshape(image, (self.inp_h, self.inp_w, 1))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "            return image, img_name, idx\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        # print(image.shape)\n",
    "\n",
    "        return image, img_name, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image --> tensor([[[246, 246, 249,  ..., 254, 251, 251],\n",
      "         [252, 246, 248,  ..., 252, 252, 252],\n",
      "         [254, 254, 255,  ..., 255, 255, 255],\n",
      "         ...,\n",
      "         [250, 255, 255,  ..., 248, 242, 242],\n",
      "         [250, 252, 252,  ..., 246, 247, 247],\n",
      "         [255, 254, 249,  ..., 254, 254, 254]]], dtype=torch.uint8)\n",
      "img_name --> 0.jpg\n",
      "idx --> 0\n"
     ]
    }
   ],
   "source": [
    "# Path to dataset\n",
    "DATA_PATH = \"data/Bangla_writing_hnhtrd/\"\n",
    "\n",
    "\n",
    "# Albumentations noise\n",
    "data_transform = A.Compose([\n",
    "    A.ElasticTransform(alpha=0.5, sigma=0, alpha_affine=0, p=0.3),\n",
    "    A.augmentations.transforms.GaussNoise(var_limit=(\n",
    "        120.0, 135.0), mean=0, always_apply=False, p=0.6),\n",
    "    A.augmentations.transforms.MotionBlur(blur_limit=(3, 6), p=0.3),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = OCRDataset(os.path.join(\n",
    "    DATA_PATH, \"train_img\"), transform=data_transform)\n",
    "\n",
    "for image, img_name, idx in train_dataset:\n",
    "    print(\"Image -->\", image)\n",
    "    print(\"img_name -->\", img_name)\n",
    "    print(\"idx -->\", idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
